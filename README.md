Hi there! I am **Muntaqim Ahmed**, a PhD student interested in **Multimodal Machine Learning**, **Medical Imaging**, **LLMs/VLMs**, **Generative AI**, and **Time-Series**.

My research centers on developing models that reason jointly over **multiple modalities** — including medical images, clinical text, and physiological time-series signals — with applications in **healthcare**. I am particularly interested in how multimodal architectures can be designed to improve performance, robustness, and fairness across diverse and clinically meaningful data sources.

For those who may be new to this area, *multimodal machine learning* studies how to integrate information from different data types. 

To gain a foundational understanding of multimodal ML, I highly recommend the survey:  
**“A Comprehensive Survey of Multimodal Machine Learning”** — https://arxiv.org/pdf/1705.09406

---

### Experience

In the **multimodal & deep learning** realm, I have experience with image–text fusion models, VLMs (e.g., CLIP-style architectures), multimodal transformers, and generative models for synthetic data.

In the **medical imaging** realm, I work on radiology-focused multimodal classifiers, fairness assessments of clinical VLMs, and evaluation of diagnostic pipelines.

In the **healthcare time-series** realm, I study physiological signals, analyze raw vs. decomposed representations, and develop forecasting models for clinical and human-performance applications.

In the **engineering & programming** realm, I work extensively with Python, PyTorch, TensorFlow/Keras, NumPy, and scientific computing tools.

---

You can reach me at **muntaqimahmed@gmail.com**
